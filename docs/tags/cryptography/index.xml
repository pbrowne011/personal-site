<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>cryptography on Pat Browne</title>
    <link>https://patbrowne.com/tags/cryptography/</link>
    <description>Recent content in cryptography on Pat Browne</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Sep 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://patbrowne.com/tags/cryptography/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Understanding learning with errors</title>
      <link>https://patbrowne.com/posts/lwe/</link>
      <pubDate>Thu, 19 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://patbrowne.com/posts/lwe/</guid>
      <description>Cryptographic protocols that perform computations on encrypted data without decrypting it are known as homomorphic encryption. Fully homomorphic encryption (FHE) protocols, systems which support arbitrary computations, often rely on different primitives than conventional cryptography systems. Most FHE systems in practice used lattice-based cryptography, relying on a computational hardness assumption known as the learning with errors (LWE) problem.
Cryptograhy relies on assumptions Prior to the 20th century, cryptography did not rely on a formalized system of provable security. Rather, one set of people (cryptographers) would come up with clever ciphers, and another set of people (cryptanalysts) would attempt to break those ciphers. Encryption schemes such as the Caesar cipher, the Vingen√®re cipher, the Enigma, and other systems were not formally proven secure by cryptographers. Rather, they relied on obfuscation and statistical properties that made deciphering messages fairly difficult. However, each one of these systems had some sort of flaw that cryptanalysts were able to exploit to decipher encrypted messages. 1
Modern cryptography, starting in the 1970s and 80s, changed the way we define security to be about adversarial capabilities rather than strategies. Instead of attempting to figure out the many ways a potential adversary could attack a specific protocol, cryptographers base security protocols on well-studied problems for which no known solution exists. The fundamental assumption that much of cryptography rests on is that $P \neq NP$.
Why is this a bedrock assumption for many protocols? The definitions that cryptography uses today all come from security games, where an arbitrary distinguisher (the adversary) tries to figure out what the challenger (the protocol) is doing.</description>
    </item>
  </channel>
</rss>
