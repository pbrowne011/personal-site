#!/usr/bin/env python3
"""
Belichick Library Data Cleaning Script

Generated by Claude Opus 4.5

Merges and cleans book data from two CSV files, generating a consolidated JSON
catalogue for the interactive library page.

Usage:
    python scripts/clean_belichick_data.py

Output:
    static/data/books.json - Cleaned and structured book data
"""

import csv
import json
import re
import urllib.parse
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional


# Abbreviation expansions
ABBREVIATIONS = {
    r'\bFB\b': 'Football',
    r'\bND\b': 'Notre Dame',
    r'\bQB\b': 'Quarterback',
    r'\bQback\b': 'Quarterback',
    r'\bRB\b': 'Running Back',
    r'\bHS\b': 'High School',
    r'\bYrs\b': 'Years',
    r'\bBk\b': 'Book',
    r'\bBldg\b': 'Building',
    r'\bCom\.\b': 'Complete',
    r'\bBal\b': 'Ball',
    r'\bOff\.\b': 'Offensive',
    r'\bDef\.\b': 'Defensive',
    r'\bvs\.\b': 'vs',
    r'\bU of\b': 'University of',
    r"\bNat'l\b": 'National',
}

# Category mappings
CATEGORY_MAP = {
    'BIO': 'Biography',
    'FH': 'Football History',
    'INST': 'Instructional',
    'NAVY': 'Navy/Military',
    'NAVAL': 'Navy/Military',
    'GS': 'General Sports',
    'inst': 'Instructional',  # Handle lowercase
}

# Stop words to exclude from keywords
STOP_WORDS = {
    'the', 'a', 'an', 'of', 'and', 'in', 'to', 'for', 'with', 'on', 'at', 'by', 'from', 'how'
}


def expand_abbreviations(text: str) -> str:
    """Expand common abbreviations in text."""
    if not text:
        return text
    for pattern, replacement in ABBREVIATIONS.items():
        text = re.sub(pattern, replacement, text)
    return text


def clean_title(title: str) -> tuple[str, int]:
    """
    Clean and normalize a book title.
    Returns (cleaned_title, copy_count).
    """
    if not title or not title.strip():
        return '', 1

    title = title.strip()

    # Extract copy count (e.g., "(2)", "(3)", "(4)")
    copy_match = re.search(r'\s*\((\d+)\)\s*$', title)
    copies = 1
    if copy_match:
        copies = int(copy_match.group(1))
        title = title[:copy_match.start()].strip()

    # Expand abbreviations
    title = expand_abbreviations(title)

    # Remove trailing periods
    title = title.rstrip('.')

    return title, copies


def normalize_author_name(name: str) -> str:
    """Convert 'Last, First' format to 'First Last'."""
    if not name:
        return name

    # Handle "Last, First" format
    if ',' in name and ' and ' not in name.lower() and ' & ' not in name:
        parts = name.split(',', 1)
        if len(parts) == 2:
            last = parts[0].strip()
            first = parts[1].strip()
            return f"{first} {last}"

    return name


def clean_author(author: str) -> tuple[str, str]:
    """
    Clean and normalize author name(s).
    Returns (display_name, sort_name).
    """
    if not author or author.strip().lower() in ('na', 'n/a', ''):
        return '', ''

    author = author.strip()

    # Handle parenthetical co-authors like "Steve Wright (William Gildea, Kenneth Turan)"
    paren_match = re.match(r'^(.+?)\s*\(([^)]+)\)$', author)
    if paren_match:
        primary = paren_match.group(1).strip()
        secondary = paren_match.group(2).strip()
        primary = normalize_author_name(primary)
        return f"{primary} ({secondary})", get_sort_name(primary)

    # Handle "with" authors
    if ' with ' in author.lower():
        parts = re.split(r'\s+with\s+', author, flags=re.IGNORECASE)
        if len(parts) >= 2:
            primary = normalize_author_name(parts[0].strip())
            secondary = normalize_author_name(parts[1].strip())
            return f"{primary} with {secondary}", get_sort_name(primary)

    # Handle multiple authors with " and ", " & ", or "/"
    for sep in [' and ', ' & ', '/']:
        if sep in author:
            authors = author.split(sep)
            normalized = [normalize_author_name(a.strip()) for a in authors]
            primary = normalized[0] if normalized else ''
            return sep.join(normalized), get_sort_name(primary)

    # Single author
    author = normalize_author_name(author)
    return author, get_sort_name(author)


def get_sort_name(author: str) -> str:
    """Get sortable name (Last, First) from display name."""
    if not author:
        return ''

    # Already in "Last, First" format
    if ',' in author:
        return author

    # Split into parts
    parts = author.split()
    if len(parts) >= 2:
        # Handle "Jr." or "Sr." suffixes
        if parts[-1] in ('Jr.', 'Sr.', 'III', 'II'):
            if len(parts) >= 3:
                return f"{parts[-2]}, {' '.join(parts[:-2])} {parts[-1]}"
        return f"{parts[-1]}, {' '.join(parts[:-1])}"

    return author


def clean_year(year_str: str) -> tuple[Optional[int], str]:
    """
    Clean and parse year value.
    Returns (year_int_or_none, display_string).
    """
    if not year_str:
        return None, ''

    year_str = str(year_str).strip()

    # Handle na/NA
    if year_str.lower() in ('na', 'n/a', ''):
        return None, ''

    # Handle ranges like "1967,69" - use first year
    range_match = re.match(r'^(\d{4}),\d{2}$', year_str)
    if range_match:
        year = int(range_match.group(1))
        return year, str(year)

    # Handle partial years like "192?"
    partial_match = re.match(r'^(\d{3})\?$', year_str)
    if partial_match:
        decade = partial_match.group(1)
        return int(decade + '0'), f"{decade}0s"

    # Try to parse as integer
    try:
        year = int(year_str)
        if 1800 <= year <= 2100:
            return year, str(year)
    except ValueError:
        pass

    return None, year_str if year_str else ''


def clean_publisher(publisher: str) -> Optional[str]:
    """Clean and normalize publisher name."""
    if not publisher or publisher.strip().lower() in ('na', 'n/a', ''):
        return None

    publisher = publisher.strip()

    # Standardize common variations
    publisher = re.sub(r'\s+INC\.?$', '', publisher, flags=re.IGNORECASE)
    publisher = re.sub(r'\s+Inc\.?$', '', publisher, flags=re.IGNORECASE)
    publisher = re.sub(r'\s+and\s+Company', ' & Co.', publisher, flags=re.IGNORECASE)
    publisher = re.sub(r'\s+&\s+Company', ' & Co.', publisher, flags=re.IGNORECASE)

    return publisher if publisher else None


def parse_location(loc_str: str) -> Optional[dict]:
    """Parse location code like 'C3S5' into structured format."""
    if not loc_str:
        return None

    loc_str = loc_str.strip().upper()
    match = re.match(r'^C(\d+)S(\d+)$', loc_str)
    if match:
        return {'case': int(match.group(1)), 'shelf': int(match.group(2))}

    # Handle variant like "C7C2" (typo for C7S2)
    match = re.match(r'^C(\d+)C(\d+)$', loc_str)
    if match:
        return {'case': int(match.group(1)), 'shelf': int(match.group(2))}

    return None


def map_category(code: str) -> tuple[Optional[str], Optional[str]]:
    """Map category code to human-readable name. Returns (name, code)."""
    if not code:
        return None, None

    code = code.strip().upper()
    if code.lower() == 'inst':
        code = 'INST'

    name = CATEGORY_MAP.get(code)
    if name:
        return name, code

    # Handle 'Naval' as variant of 'NAVY'
    if code == 'NAVAL':
        return 'Navy/Military', 'NAVY'

    return None, None


def generate_keywords(title: str, author: str) -> list[str]:
    """Generate search keywords from title and author."""
    keywords = set()

    # Extract words from title
    if title:
        words = re.findall(r'\b[a-zA-Z]+\b', title.lower())
        for word in words:
            if word not in STOP_WORDS and len(word) > 1:
                keywords.add(word)

    # Extract words from author
    if author:
        # Remove "with" and parenthetical parts for keyword extraction
        author_clean = re.sub(r'\s+with\s+.*$', '', author, flags=re.IGNORECASE)
        author_clean = re.sub(r'\s*\([^)]+\)', '', author_clean)
        words = re.findall(r'\b[a-zA-Z]+\b', author_clean.lower())
        for word in words:
            if word not in STOP_WORDS and len(word) > 1:
                keywords.add(word)

    return sorted(keywords)


def generate_links(title: str, author: str) -> dict:
    """Generate external resource links for a book."""
    # Clean inputs for URL encoding
    title_clean = title.strip() if title else ''

    # Use primary author only
    author_clean = author.strip() if author else ''
    if ' with ' in author_clean.lower():
        author_clean = re.split(r'\s+with\s+', author_clean, flags=re.IGNORECASE)[0].strip()

    # Remove parenthetical parts
    author_clean = re.sub(r'\s*\([^)]+\)', '', author_clean).strip()

    # Build search query
    if author_clean:
        search_query = f"{title_clean} {author_clean}"
    else:
        search_query = title_clean

    encoded_query = urllib.parse.quote(search_query)

    return {
        'worldcat': f"https://search.worldcat.org/search?q={encoded_query}",
        'internet_archive': f"https://archive.org/search?query={encoded_query}",
        'annas_archive': f"https://annas-archive.org/search?q={encoded_query}",
        'bookfinder': f"https://www.bookfinder.com/search/?keywords={encoded_query}&currency=USD&destination=us&mode=basic&classic=off&ps=tp&order=pricedesc&st=sr&ac=qr"
    }


def generate_id(title: str, author: str, year: Optional[int]) -> str:
    """Generate URL-safe ID from title, author, and year."""
    # Get primary author last name
    author_part = ''
    if author:
        author_clean = author.strip()
        if ' with ' in author_clean.lower():
            author_clean = re.split(r'\s+with\s+', author_clean, flags=re.IGNORECASE)[0]
        author_clean = re.sub(r'\s*\([^)]+\)', '', author_clean).strip()

        # Get last name
        parts = author_clean.split()
        if parts:
            # Handle "Last, First" format
            if ',' in parts[0]:
                author_part = parts[0].rstrip(',')
            else:
                author_part = parts[-1]

    # Create slug
    def slugify(text):
        text = text.lower()
        text = re.sub(r'[^a-z0-9\s-]', '', text)
        text = re.sub(r'[\s_]+', '-', text)
        text = re.sub(r'-+', '-', text)
        return text.strip('-')

    title_slug = slugify(title)[:50] if title else 'untitled'
    author_slug = slugify(author_part)[:20] if author_part else ''

    if author_slug and year:
        return f"{title_slug}-{author_slug}-{year}"
    elif author_slug:
        return f"{title_slug}-{author_slug}"
    elif year:
        return f"{title_slug}-{year}"
    else:
        return title_slug


def parse_csv_with_publisher(filepath: Path) -> list[dict]:
    """Parse the belichick-collection-with-publisher.csv file."""
    books = []

    with open(filepath, 'r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        header = next(reader)  # Skip header

        for row in reader:
            # Skip empty rows
            if not row or all(not cell.strip() for cell in row):
                continue

            # Pad row to expected length
            while len(row) < 8:
                row.append('')

            title_raw = row[0].strip()
            author_raw = row[1].strip()
            publisher_raw = row[2].strip()
            year_raw = row[3].strip()
            loc1 = row[4].strip() if len(row) > 4 else ''
            cat1 = row[5].strip() if len(row) > 5 else ''
            loc2 = row[6].strip() if len(row) > 6 else ''
            cat2 = row[7].strip() if len(row) > 7 else ''

            # Skip rows with no title
            if not title_raw:
                continue

            title, copies = clean_title(title_raw)
            author, author_sort = clean_author(author_raw)
            year, year_display = clean_year(year_raw)
            publisher = clean_publisher(publisher_raw)

            # Parse locations
            locations = []
            loc = parse_location(loc1)
            if loc:
                locations.append(loc)
            loc = parse_location(loc2)
            if loc and loc not in locations:
                locations.append(loc)

            # Get category (prefer first non-empty)
            category, category_code = map_category(cat1)
            if not category:
                category, category_code = map_category(cat2)

            if title:  # Only add if we have a title
                books.append({
                    'title': title,
                    'author': author,
                    'author_sort': author_sort,
                    'publisher': publisher,
                    'year': year,
                    'year_display': year_display,
                    'category': category,
                    'category_code': category_code,
                    'locations': locations,
                    'copies': copies,
                    'source': 'publisher_csv'
                })

    return books


def parse_csv_atd(filepath: Path) -> list[dict]:
    """Parse the belichick-collection-atd.csv file."""
    books = []

    with open(filepath, 'r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        header = next(reader)  # Skip header

        for row in reader:
            # Skip empty rows
            if not row or all(not cell.strip() for cell in row):
                continue

            # Pad row to expected length
            while len(row) < 8:
                row.append('')

            author_raw = row[0].strip()
            title_raw = row[1].strip()
            year_raw = row[2].strip()
            loc1 = row[3].strip() if len(row) > 3 else ''
            cat1 = row[4].strip() if len(row) > 4 else ''
            loc2 = row[5].strip() if len(row) > 5 else ''
            cat2 = row[6].strip() if len(row) > 6 else ''

            # Skip rows with no title
            if not title_raw:
                continue

            title, copies = clean_title(title_raw)
            author, author_sort = clean_author(author_raw)
            year, year_display = clean_year(year_raw)

            # Parse locations
            locations = []
            loc = parse_location(loc1)
            if loc:
                locations.append(loc)
            loc = parse_location(loc2)
            if loc and loc not in locations:
                locations.append(loc)

            # Get category
            category, category_code = map_category(cat1)
            if not category:
                category, category_code = map_category(cat2)

            if title:  # Only add if we have a title
                books.append({
                    'title': title,
                    'author': author,
                    'author_sort': author_sort,
                    'publisher': None,  # ATD file has no publisher
                    'year': year,
                    'year_display': year_display,
                    'category': category,
                    'category_code': category_code,
                    'locations': locations,
                    'copies': copies,
                    'source': 'atd_csv'
                })

    return books


def create_dedup_key(book: dict) -> str:
    """Create a normalized key for deduplication."""
    title = book.get('title', '').lower()
    title = re.sub(r'[^a-z0-9]', '', title)

    author = book.get('author', '') or ''
    author_sort = book.get('author_sort', '') or author

    # Get first author's last name
    if author_sort:
        # Handle "Last, First" format
        if ',' in author_sort:
            last_name = author_sort.split(',')[0].strip().lower()
        else:
            parts = author_sort.split()
            last_name = parts[-1].lower() if parts else ''
    else:
        last_name = ''

    last_name = re.sub(r'[^a-z]', '', last_name)

    return f"{title}_{last_name}"


def merge_books(book1: dict, book2: dict) -> dict:
    """Merge two book records, preferring non-null values."""
    merged = book1.copy()

    # Prefer more complete title
    if len(book2.get('title', '') or '') > len(merged.get('title', '') or ''):
        merged['title'] = book2['title']

    # Prefer non-empty author
    if not merged.get('author') and book2.get('author'):
        merged['author'] = book2['author']
        merged['author_sort'] = book2.get('author_sort', '')

    # Prefer publisher data (only in publisher CSV)
    if not merged.get('publisher') and book2.get('publisher'):
        merged['publisher'] = book2['publisher']

    # Prefer non-null year
    if not merged.get('year') and book2.get('year'):
        merged['year'] = book2['year']
        merged['year_display'] = book2.get('year_display', '')

    # Prefer category
    if not merged.get('category') and book2.get('category'):
        merged['category'] = book2['category']
        merged['category_code'] = book2.get('category_code')

    # Combine locations
    existing_locs = merged.get('locations', [])
    for loc in book2.get('locations', []):
        if loc not in existing_locs:
            existing_locs.append(loc)
    merged['locations'] = existing_locs

    # Keep highest copy count
    merged['copies'] = max(merged.get('copies', 1), book2.get('copies', 1))

    return merged


def deduplicate_books(books: list[dict]) -> list[dict]:
    """Deduplicate books using normalized keys."""
    seen = {}

    for book in books:
        key = create_dedup_key(book)

        if key in seen:
            # Merge with existing
            seen[key] = merge_books(seen[key], book)
        else:
            seen[key] = book

    return list(seen.values())


def main():
    """Main entry point."""
    script_dir = Path(__file__).parent
    project_root = script_dir.parent

    # Input files
    csv_publisher = project_root / 'belichick-collection-with-publisher.csv'
    csv_atd = project_root / 'belichick-collection-atd.csv'

    # Output file
    output_dir = project_root / 'static' / 'data'
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / 'books.json'

    print("Parsing CSV files...")

    # Parse both files
    books_publisher = parse_csv_with_publisher(csv_publisher)
    print(f"  - Found {len(books_publisher)} books in publisher CSV")

    books_atd = parse_csv_atd(csv_atd)
    print(f"  - Found {len(books_atd)} books in ATD CSV")

    # Combine all books
    all_books = books_publisher + books_atd
    print(f"  - Total before deduplication: {len(all_books)}")

    # Deduplicate
    print("\nDeduplicating...")
    books = deduplicate_books(all_books)
    print(f"  - Total after deduplication: {len(books)}")

    # Add generated fields
    print("\nGenerating IDs, keywords, and links...")
    for book in books:
        book['id'] = generate_id(book['title'], book['author'], book.get('year'))
        book['keywords'] = generate_keywords(book['title'], book['author'])
        book['links'] = generate_links(book['title'], book['author'])

        # Remove source field
        book.pop('source', None)

    # Sort by title
    books.sort(key=lambda b: (b.get('title', '') or '').lower())

    # Generate output
    output = {
        'metadata': {
            'generated': datetime.now(timezone.utc).isoformat(),
            'total_books': len(books),
            'source': 'USNA Belichick Collection'
        },
        'books': books
    }

    # Write JSON
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)

    print(f"\nWrote {len(books)} books to {output_file}")

    # Print statistics
    print("\n=== Statistics ===")

    # Missing data
    missing_year = sum(1 for b in books if not b.get('year'))
    missing_author = sum(1 for b in books if not b.get('author'))
    missing_publisher = sum(1 for b in books if not b.get('publisher'))

    print(f"\nMissing data:")
    print(f"  - Books without year: {missing_year}")
    print(f"  - Books without author: {missing_author}")
    print(f"  - Books without publisher: {missing_publisher}")

    # Category distribution
    print(f"\nCategory distribution:")
    categories = {}
    for book in books:
        cat = book.get('category') or 'Uncategorized'
        categories[cat] = categories.get(cat, 0) + 1
    for cat, count in sorted(categories.items(), key=lambda x: -x[1]):
        print(f"  - {cat}: {count}")

    # Decade distribution
    print(f"\nDecade distribution:")
    decades = {}
    for book in books:
        if book.get('year'):
            decade = (book['year'] // 10) * 10
            decades[decade] = decades.get(decade, 0) + 1
    for decade, count in sorted(decades.items()):
        print(f"  - {decade}s: {count}")

    print("\nDone!")


if __name__ == '__main__':
    main()
